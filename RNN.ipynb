{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3Ok95c0tyDOkJCQO+ihke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptasim8/Thesis-Related/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yi9a5LKenHBi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "# import os\n",
        "# os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nfr_bt-enV3u",
        "outputId": "f818fc87-b953-4a79-f86c-81e4f7fa6654"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQtbV9SWnZ7Y",
        "outputId": "dc409cae-1d86-4789-9073-ec5b6dad0bcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(devices[0], True)\n",
        "    print(\"Success\")\n",
        "except:\n",
        "    print(\"Exception occured\")\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3VlEGQzn3nK",
        "outputId": "c3a52b80-a17a-4275-c39f-111de11623a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('imdb_reviews', data_dir='.', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgVcPD7yn-uj",
        "outputId": "bfb65973-901e-43a9-bfa7-bd824c4f25a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ./imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to ./imdb_reviews/plain_text/1.0.0.incompleteO8NZQT/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to ./imdb_reviews/plain_text/1.0.0.incompleteO8NZQT/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to ./imdb_reviews/plain_text/1.0.0.incompleteO8NZQT/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to ./imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMTAvBB3oUlG",
        "outputId": "ed965e6e-9e80-4801-8d0d-eca6f64223a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifLYPX7uocbc",
        "outputId": "44474138-f1b3-475b-9b14-c84547812d64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
              " 'train': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
              " 'unsupervised': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "metadata": {
        "id": "t0InR08XopOe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNYAgvCrosP8",
        "outputId": "b84417f6-0ed6-46dc-f98c-3107d4b36719"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjzOMlUiox3u",
        "outputId": "11027c42-c21c-4809-8525-3318ca4a627a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8HFHXPCo1ZW",
        "outputId": "6be6be6f-ecf0-4199-fae9-76074df55f6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_dataset:\n",
        "    print(sample[0].numpy())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbXwl3fApB91",
        "outputId": "5e82bca1-0922-4fe6-9f1d-36a773fac54a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "IILuMc1cpSI0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "0bpYwBWl-GC4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print('texts: ', example.numpy()[:3])\n",
        "    print()\n",
        "    print('labels: ', label.numpy()[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdaPNKEW-Ma5",
        "outputId": "b30a781d-f03e-4a8e-a0f2-114f1059d181"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b\"Well - when the cameo appearance of Jason Miller (looking even more eroded than he did in Exorcist IV) is the high point of a picture, what've you got?<br /><br />It's a little bit country, a little bit rock n' roll: mix two drunks with money who drag their kid all over the place with a bog-dried mummy (have you figured that one out yet - DRIED in a bog?) in the basement, Christopher Walken with a bad dye job, and a little girl who might have been an interesting character if they'd developed her.<br /><br />I understand - sort of - that they're going back to visit her relatives. After that....<br /><br />Problem: There are several interesting flashbacks to what I must assume is her mother being killed in a car bombing (I think). This is never connected to anything. <br /><br />Problem: What do we need the grandmother for? Now, the grandmother could be interesting. She speaks Gaelic, or Celtic, or something. Maybe you can make something of her. The best they can do is that she 's got a tobacco habit. That's all.<br /><br />Problem: They cast a real shifty character as the husband. Is he type-cast (will he sell his wife to the devil? Maybe he can look forward to the trust fund he manages for her)or is he cast against type (after all, he has a good haircut and nice clothes)? He drinks, he hesitates. He's not a bad guy. Not a good one. But dislikable. Why didn't they DO something with him?<br /><br />No problem: an old boyfriend shows up. The husband knocks him down. He comes back to knock down the husband. (It gets pretty stupid, but at least THAT character has motivation.) <br /><br />NOW - she's an alcoholic, he's an alcoholic; he might only have married her for her money. The grandmother is locked in the bedroom. The blind uncle takes our heroine to the basement to show her the mummy of a witch (are you following this?) who may come to life. In fact, you KNOW she'll come to life, the music swells. A little girl lives in the house, takes tea to the grandmother (unlocks the door to do so) and provides granny with cigarettes. Periodically, granny gets out. But nothing happens. <br /><br />Husband and wife lose the kid in the house, subsequently lose their bedroom. Uncle gets his throat cut in the basement. The leading lady has nose-bleeds. The husband drinks. They both drink. In the face of all of this, the awful truth alluded to in the first over-voice is - omigod - an abortion when the leading lady was twelve years old.<br /><br />In spite of all these dangling-thread ingredients, nobody managed to get a story on the screen. No bridge between situations, no graduation from mild disturbance to awful horror, just long slow scenes that go nowhere.;nbody, really, to care about - and they had places to go with that aspect - the innocent kid in the charge of drunks,the grandmother who might be locked up because she's a monster, but no, her worst fault is smoking. She's got great hair, good makeup. <br /><br />In short, no plot. Just a little random (predictable)violence in a dark library, with the rain gushing in, and the sound track cuing us in. You need more than a few drunks and Christopher Walken to make a movie.<br /><br />The production values were good. Oh. Nice scenery, good wardrobe. The cameraman, at least, knew what he was doing.<br /><br />I bought it. Poor me.\"\n",
            " b\"A comedy of epically funny proportions from the guys that brought you South Park, and most of the guys from Orgazmo. This vulgur, obscence movie has utterly disgusting, eggotistical, and satirical content. It portrays incredibly cruel treatment of humans and animals. I LOVE IT!!!!! This is some funny stuff. Really funny. Two loser friends create a game in thier driveway, which explodes into a national sensation. Corruption and greed and blackmail turn the sport sour, and its up ta Coop ta fix it. And along the way, you will laugh. Alot. That's all there is. Enjoy!!!!\"\n",
            " b'I am amazed at the amount of praise that is heaped on this movie by other commentators. To me it was rather a disappointment, especially the combination of historical facts, fantasy and the main character\\'s internal turmoil does not work at all (in Vonnegut\\'s book Slaughterhouse Five and even in George Roy Hill\\'s adaptation for the screen it does). Credibility is often overstretched. Too many questions are left open. Did I miss some central points? Or did I fail to spot the lines that supposedly connect the dots? <br /><br />A boy called Campbell, Jr., grows up in upstate New York. At home his father has many technical trade papers and one book. It has photographs of heaps of dead bodies in it. The boy leafs through the book, his dad doesn\\'t like his doing that. What should this tell me? The family moves away from upstate New York to Berlin. BANG. It is 1938, the boy is a married man in Berlin and a theater playwright. What kind of plays does he write? In what language? Is he successful? His wife is an actress and looks glamorous. The parents move back to the USA and invite their son to do the same. He does not. Why? Because having grown up in Germany he feels more German than American? Because he is successful? Because his wife is? Because he likes his life there? Because he likes the Nazis? Because he is just plain lazy and doesn\\'t like change? Don\\'t ask me.<br /><br />Possibly, the man just does not care, is not interested in politics, is a kind of an existentialist. He states that he is deeply in love with his wife. He speaks of his Republic of Two (meaning he and his wife). There is little to no evidence proving his love for his wife in the movie, it much more seems a Republic of One.<br /><br />On the request of an American agent Campbell, Jr., agrees to broadcast anti Semitic Nazi hate propaganda to American listeners as a device for transmitting encrypted messages to American authorities who read between the lines. The crucial meeting with the agent on a Berlin park bench is short, unexciting and anti climactic, the decision to play along comes pretty easily with no explanation, the rise up to broadcaster seems to be uneventful and apparently fast.<br /><br />So now we have Campbell, Jr., presenting himself over the air as the Last Free American. The scheme for transmitting secret messages is fairly realistic and exciting - although one wonders what happened when Campbell, Jr., really and honestly had to cough, hiccup etc. (must have scrambled the messages terribly). Anyway, the Nazis lose, the wife dies (touring in the Crimean for German troops - I never heard such tours really happened on German front lines in WW II), Campbell, Jr., says he goes to the Russian front but does not go, is captured by an American soldier who recognizes his mug (how come?), is dragged to a sight-seeing tour in Auschwitz, is then released and resettled with the help of the Crucial Agent somewhere in the City of New York.<br /><br />AND THIS IS WHERE THE STORY REALLY STARTS <br /><br />BANG. From now on it is like a short story by Paul Auster. It is 1961, Campbell, Jr., lives in New York tenement as a has-been and mourns the loss of his wife. Nobody really cares - or do they? Yes, somehow they do, and his neighbors offer some sort of distraction. Auschwitz survivors. A painter. Some American supremacists \\xc2\\x84discover\" him and want him to be their figurehead. They even find his presumed dead wife for him, or is she his wife? Anyway, in the end Campbell, Jr., calls in at the Israeli consulate, and they obligingly give him the Big War Criminal treatment, placing him in the cell adjacent to Adolf Eichmann\\'s. He writes his life story and, once this task finished, hangs himself on the typewriter\\'s ribbons without getting sooty the least bit.<br /><br />While I can see that there must be an issue of guilt and of loss, I just had the impression that the main character is a person who at all times is pretty indifferent to everything and hardly capable of love for anyone. So I found it difficult to sympathize for this looser who mourns his loss. Amazingly, many reviewers focus on his status as a potential war hero, having put his reputation at stake for playing the Last Free American. I assume according to them this took a lot of courage. As a matter of fact, however, the movie suggests that by accepting the assignment Campbell created for himself a win-win situation, as he would have been politically on the safe side no matter who had won the war. The danger of his being uncovered never comes up during the first part of the story.<br /><br />One might argue, that the whole story is a dreamlike fantasy and that nobody should bother with historical accuracy or a logical development of the story which explains everything. But even then it fails to make a point, primarily, I suspect, because the love affair in the Republic of Two falls completely flat. This is a pity, especially if you consider that the wife was played by Sheryl Lee, a talented, versatile and sensuous actress. She has much too little screen time and is forced to use a ridiculous German accent. Another somehow neglected aspect are the different texts (confession, broadcast and hidden messages), but I guess this is largely unfilmable. Maybe I should give the book a chance.']\n",
            "\n",
            "labels:  [0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = tf.keras.layers.experimental.preprocessing.TextVectorization()\n",
        "e.adapt([\n",
        "    \"I love samosas and jalebi\",\n",
        "    \"I love biking and yoga\",\n",
        "    \"I love tensorflow\"\n",
        "])"
      ],
      "metadata": {
        "id": "9SED5CgG-XMU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSihqN-s-d5P",
        "outputId": "7cdc122c-c55e-4d99-f8c8-3de41f372bff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'love',\n",
              " 'i',\n",
              " 'and',\n",
              " 'yoga',\n",
              " 'tensorflow',\n",
              " 'samosas',\n",
              " 'jalebi',\n",
              " 'biking']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e([\"I love pizza\"]).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DylrLbGX-lfK",
        "outputId": "eb94bb3f-ae5d-47be-e4c7-05b63c8551c5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "HQeU6jb4-odP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh3q_VPp-sUM",
        "outputId": "29b6cbf4-92c6-43ff-aec9-783e8dbf4050"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but',\n",
              "       'film', 'on', 'not', 'you', 'are'], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLVsmWRC-v7I",
        "outputId": "c86cf94c-6d78-46e9-cecb-6f317e90c497"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b\"Well - when the cameo appearance of Jason Miller (looking even more eroded than he did in Exorcist IV) is the high point of a picture, what've you got?<br /><br />It's a little bit country, a little bit rock n' roll: mix two drunks with money who drag their kid all over the place with a bog-dried mummy (have you figured that one out yet - DRIED in a bog?) in the basement, Christopher Walken with a bad dye job, and a little girl who might have been an interesting character if they'd developed her.<br /><br />I understand - sort of - that they're going back to visit her relatives. After that....<br /><br />Problem: There are several interesting flashbacks to what I must assume is her mother being killed in a car bombing (I think). This is never connected to anything. <br /><br />Problem: What do we need the grandmother for? Now, the grandmother could be interesting. She speaks Gaelic, or Celtic, or something. Maybe you can make something of her. The best they can do is that she 's got a tobacco habit. That's all.<br /><br />Problem: They cast a real shifty character as the husband. Is he type-cast (will he sell his wife to the devil? Maybe he can look forward to the trust fund he manages for her)or is he cast against type (after all, he has a good haircut and nice clothes)? He drinks, he hesitates. He's not a bad guy. Not a good one. But dislikable. Why didn't they DO something with him?<br /><br />No problem: an old boyfriend shows up. The husband knocks him down. He comes back to knock down the husband. (It gets pretty stupid, but at least THAT character has motivation.) <br /><br />NOW - she's an alcoholic, he's an alcoholic; he might only have married her for her money. The grandmother is locked in the bedroom. The blind uncle takes our heroine to the basement to show her the mummy of a witch (are you following this?) who may come to life. In fact, you KNOW she'll come to life, the music swells. A little girl lives in the house, takes tea to the grandmother (unlocks the door to do so) and provides granny with cigarettes. Periodically, granny gets out. But nothing happens. <br /><br />Husband and wife lose the kid in the house, subsequently lose their bedroom. Uncle gets his throat cut in the basement. The leading lady has nose-bleeds. The husband drinks. They both drink. In the face of all of this, the awful truth alluded to in the first over-voice is - omigod - an abortion when the leading lady was twelve years old.<br /><br />In spite of all these dangling-thread ingredients, nobody managed to get a story on the screen. No bridge between situations, no graduation from mild disturbance to awful horror, just long slow scenes that go nowhere.;nbody, really, to care about - and they had places to go with that aspect - the innocent kid in the charge of drunks,the grandmother who might be locked up because she's a monster, but no, her worst fault is smoking. She's got great hair, good makeup. <br /><br />In short, no plot. Just a little random (predictable)violence in a dark library, with the rain gushing in, and the sound track cuing us in. You need more than a few drunks and Christopher Walken to make a movie.<br /><br />The production values were good. Oh. Nice scenery, good wardrobe. The cameraman, at least, knew what he was doing.<br /><br />I bought it. Poor me.\">"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCSleuBR-zOf",
        "outputId": "3aa983e3-13b5-4f3a-ceb5-c9a61f0ddccd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 74,  51,   2, ...,   0,   0,   0],\n",
              "       [  4, 220,   5, ...,   0,   0,   0],\n",
              "       [ 10, 237,   1, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(3):\n",
        "    print(\"Original: \", example[n].numpy())\n",
        "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RkXb2dH_CgE",
        "outputId": "f8d3c072-0d06-469b-df97-ee57a0866baa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"Well - when the cameo appearance of Jason Miller (looking even more eroded than he did in Exorcist IV) is the high point of a picture, what've you got?<br /><br />It's a little bit country, a little bit rock n' roll: mix two drunks with money who drag their kid all over the place with a bog-dried mummy (have you figured that one out yet - DRIED in a bog?) in the basement, Christopher Walken with a bad dye job, and a little girl who might have been an interesting character if they'd developed her.<br /><br />I understand - sort of - that they're going back to visit her relatives. After that....<br /><br />Problem: There are several interesting flashbacks to what I must assume is her mother being killed in a car bombing (I think). This is never connected to anything. <br /><br />Problem: What do we need the grandmother for? Now, the grandmother could be interesting. She speaks Gaelic, or Celtic, or something. Maybe you can make something of her. The best they can do is that she 's got a tobacco habit. That's all.<br /><br />Problem: They cast a real shifty character as the husband. Is he type-cast (will he sell his wife to the devil? Maybe he can look forward to the trust fund he manages for her)or is he cast against type (after all, he has a good haircut and nice clothes)? He drinks, he hesitates. He's not a bad guy. Not a good one. But dislikable. Why didn't they DO something with him?<br /><br />No problem: an old boyfriend shows up. The husband knocks him down. He comes back to knock down the husband. (It gets pretty stupid, but at least THAT character has motivation.) <br /><br />NOW - she's an alcoholic, he's an alcoholic; he might only have married her for her money. The grandmother is locked in the bedroom. The blind uncle takes our heroine to the basement to show her the mummy of a witch (are you following this?) who may come to life. In fact, you KNOW she'll come to life, the music swells. A little girl lives in the house, takes tea to the grandmother (unlocks the door to do so) and provides granny with cigarettes. Periodically, granny gets out. But nothing happens. <br /><br />Husband and wife lose the kid in the house, subsequently lose their bedroom. Uncle gets his throat cut in the basement. The leading lady has nose-bleeds. The husband drinks. They both drink. In the face of all of this, the awful truth alluded to in the first over-voice is - omigod - an abortion when the leading lady was twelve years old.<br /><br />In spite of all these dangling-thread ingredients, nobody managed to get a story on the screen. No bridge between situations, no graduation from mild disturbance to awful horror, just long slow scenes that go nowhere.;nbody, really, to care about - and they had places to go with that aspect - the innocent kid in the charge of drunks,the grandmother who might be locked up because she's a monster, but no, her worst fault is smoking. She's got great hair, good makeup. <br /><br />In short, no plot. Just a little random (predictable)violence in a dark library, with the rain gushing in, and the sound track cuing us in. You need more than a few drunks and Christopher Walken to make a movie.<br /><br />The production values were good. Oh. Nice scenery, good wardrobe. The cameraman, at least, knew what he was doing.<br /><br />I bought it. Poor me.\"\n",
            "Round-trip:  well when the [UNK] [UNK] of [UNK] [UNK] looking even more [UNK] than he did in [UNK] [UNK] is the high point of a picture [UNK] you [UNK] br its a little bit country a little bit rock [UNK] [UNK] [UNK] two [UNK] with money who [UNK] their kid all over the place with a [UNK] [UNK] have you [UNK] that one out yet [UNK] in a [UNK] in the [UNK] [UNK] [UNK] with a bad [UNK] job and a little girl who might have been an interesting character if [UNK] [UNK] [UNK] br i understand sort of that theyre going back to [UNK] her [UNK] after [UNK] br problem there are several interesting [UNK] to what i must [UNK] is her mother being killed in a car [UNK] i think this is never [UNK] to anything br br problem what do we need the [UNK] for now the [UNK] could be interesting she [UNK] [UNK] or [UNK] or something maybe you can make something of her the best they can do is that she [UNK] got a [UNK] [UNK] thats [UNK] br problem they cast a real [UNK] character as the husband is he [UNK] will he [UNK] his wife to the [UNK] maybe he can look forward to the [UNK] [UNK] he manages for [UNK] is he cast against type after all he has a good [UNK] and nice [UNK] he [UNK] he [UNK] hes not a bad guy not a good one but [UNK] why didnt they do something with [UNK] br no problem an old [UNK] shows up the husband [UNK] him down he comes back to [UNK] down the husband it gets pretty stupid but at least that character has [UNK] br br now shes an [UNK] hes an [UNK] he might only have [UNK] her for her money the [UNK] is [UNK] in the [UNK] the [UNK] [UNK] takes our [UNK] to the [UNK] to show her the [UNK] of a [UNK] are you [UNK] this who may come to life in fact you know [UNK] come to life the music [UNK] a little girl lives in the house takes [UNK] to the [UNK] [UNK] the [UNK] to do so and [UNK] [UNK] with [UNK] [UNK] [UNK] gets out but nothing happens br br husband and wife [UNK] the kid in the house [UNK] [UNK] their [UNK] [UNK] gets his [UNK] cut in the [UNK] the leading lady has [UNK] the husband [UNK] they both [UNK] in the face of all of this the awful truth [UNK] to in the first [UNK] is [UNK] an [UNK] when the leading lady was [UNK] years [UNK] br in [UNK] of all these [UNK] [UNK] [UNK] [UNK] to get a story on the screen no [UNK] between [UNK] no [UNK] from [UNK] [UNK] to awful horror just long slow scenes that go [UNK] really to care about and they had [UNK] to go with that [UNK] the [UNK] kid in the [UNK] of [UNK] [UNK] who might be [UNK] up because shes a monster but no her worst [UNK] is [UNK] shes got great [UNK] good [UNK] br br in short no plot just a little [UNK] [UNK] in a dark [UNK] with the [UNK] [UNK] in and the sound [UNK] [UNK] us in you need more than a few [UNK] and [UNK] [UNK] to make a moviebr br the production [UNK] were good oh nice [UNK] good [UNK] the [UNK] at least knew what he was [UNK] br i [UNK] it poor me                                                                                                                                                                                                                                                                                                                                                                                 \n",
            "\n",
            "Original:  b\"A comedy of epically funny proportions from the guys that brought you South Park, and most of the guys from Orgazmo. This vulgur, obscence movie has utterly disgusting, eggotistical, and satirical content. It portrays incredibly cruel treatment of humans and animals. I LOVE IT!!!!! This is some funny stuff. Really funny. Two loser friends create a game in thier driveway, which explodes into a national sensation. Corruption and greed and blackmail turn the sport sour, and its up ta Coop ta fix it. And along the way, you will laugh. Alot. That's all there is. Enjoy!!!!\"\n",
            "Round-trip:  a comedy of [UNK] funny [UNK] from the guys that brought you [UNK] [UNK] and most of the guys from [UNK] this [UNK] [UNK] movie has [UNK] [UNK] [UNK] and [UNK] [UNK] it [UNK] incredibly [UNK] [UNK] of [UNK] and [UNK] i love it this is some funny stuff really funny two [UNK] friends create a game in [UNK] [UNK] which [UNK] into a [UNK] [UNK] [UNK] and [UNK] and [UNK] turn the [UNK] [UNK] and its up [UNK] [UNK] [UNK] [UNK] it and along the way you will laugh [UNK] thats all there is enjoy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "\n",
            "Original:  b'I am amazed at the amount of praise that is heaped on this movie by other commentators. To me it was rather a disappointment, especially the combination of historical facts, fantasy and the main character\\'s internal turmoil does not work at all (in Vonnegut\\'s book Slaughterhouse Five and even in George Roy Hill\\'s adaptation for the screen it does). Credibility is often overstretched. Too many questions are left open. Did I miss some central points? Or did I fail to spot the lines that supposedly connect the dots? <br /><br />A boy called Campbell, Jr., grows up in upstate New York. At home his father has many technical trade papers and one book. It has photographs of heaps of dead bodies in it. The boy leafs through the book, his dad doesn\\'t like his doing that. What should this tell me? The family moves away from upstate New York to Berlin. BANG. It is 1938, the boy is a married man in Berlin and a theater playwright. What kind of plays does he write? In what language? Is he successful? His wife is an actress and looks glamorous. The parents move back to the USA and invite their son to do the same. He does not. Why? Because having grown up in Germany he feels more German than American? Because he is successful? Because his wife is? Because he likes his life there? Because he likes the Nazis? Because he is just plain lazy and doesn\\'t like change? Don\\'t ask me.<br /><br />Possibly, the man just does not care, is not interested in politics, is a kind of an existentialist. He states that he is deeply in love with his wife. He speaks of his Republic of Two (meaning he and his wife). There is little to no evidence proving his love for his wife in the movie, it much more seems a Republic of One.<br /><br />On the request of an American agent Campbell, Jr., agrees to broadcast anti Semitic Nazi hate propaganda to American listeners as a device for transmitting encrypted messages to American authorities who read between the lines. The crucial meeting with the agent on a Berlin park bench is short, unexciting and anti climactic, the decision to play along comes pretty easily with no explanation, the rise up to broadcaster seems to be uneventful and apparently fast.<br /><br />So now we have Campbell, Jr., presenting himself over the air as the Last Free American. The scheme for transmitting secret messages is fairly realistic and exciting - although one wonders what happened when Campbell, Jr., really and honestly had to cough, hiccup etc. (must have scrambled the messages terribly). Anyway, the Nazis lose, the wife dies (touring in the Crimean for German troops - I never heard such tours really happened on German front lines in WW II), Campbell, Jr., says he goes to the Russian front but does not go, is captured by an American soldier who recognizes his mug (how come?), is dragged to a sight-seeing tour in Auschwitz, is then released and resettled with the help of the Crucial Agent somewhere in the City of New York.<br /><br />AND THIS IS WHERE THE STORY REALLY STARTS <br /><br />BANG. From now on it is like a short story by Paul Auster. It is 1961, Campbell, Jr., lives in New York tenement as a has-been and mourns the loss of his wife. Nobody really cares - or do they? Yes, somehow they do, and his neighbors offer some sort of distraction. Auschwitz survivors. A painter. Some American supremacists \\xc2\\x84discover\" him and want him to be their figurehead. They even find his presumed dead wife for him, or is she his wife? Anyway, in the end Campbell, Jr., calls in at the Israeli consulate, and they obligingly give him the Big War Criminal treatment, placing him in the cell adjacent to Adolf Eichmann\\'s. He writes his life story and, once this task finished, hangs himself on the typewriter\\'s ribbons without getting sooty the least bit.<br /><br />While I can see that there must be an issue of guilt and of loss, I just had the impression that the main character is a person who at all times is pretty indifferent to everything and hardly capable of love for anyone. So I found it difficult to sympathize for this looser who mourns his loss. Amazingly, many reviewers focus on his status as a potential war hero, having put his reputation at stake for playing the Last Free American. I assume according to them this took a lot of courage. As a matter of fact, however, the movie suggests that by accepting the assignment Campbell created for himself a win-win situation, as he would have been politically on the safe side no matter who had won the war. The danger of his being uncovered never comes up during the first part of the story.<br /><br />One might argue, that the whole story is a dreamlike fantasy and that nobody should bother with historical accuracy or a logical development of the story which explains everything. But even then it fails to make a point, primarily, I suspect, because the love affair in the Republic of Two falls completely flat. This is a pity, especially if you consider that the wife was played by Sheryl Lee, a talented, versatile and sensuous actress. She has much too little screen time and is forced to use a ridiculous German accent. Another somehow neglected aspect are the different texts (confession, broadcast and hidden messages), but I guess this is largely unfilmable. Maybe I should give the book a chance.'\n",
            "Round-trip:  i am [UNK] at the [UNK] of [UNK] that is [UNK] on this movie by other [UNK] to me it was rather a [UNK] especially the [UNK] of [UNK] [UNK] fantasy and the main characters [UNK] [UNK] does not work at all in [UNK] book [UNK] five and even in george [UNK] [UNK] [UNK] for the screen it does [UNK] is often [UNK] too many [UNK] are left open did i miss some [UNK] points or did i [UNK] to [UNK] the lines that [UNK] [UNK] the [UNK] br br a boy called [UNK] [UNK] [UNK] up in [UNK] new york at home his father has many [UNK] [UNK] [UNK] and one book it has [UNK] of [UNK] of dead [UNK] in it the boy [UNK] through the book his [UNK] doesnt like his doing that what should this tell me the family [UNK] away from [UNK] new york to [UNK] [UNK] it is [UNK] the boy is a [UNK] man in [UNK] and a theater [UNK] what kind of plays does he write in what [UNK] is he [UNK] his wife is an actress and looks [UNK] the parents move back to the [UNK] and [UNK] their son to do the same he does not why because having [UNK] up in [UNK] he feels more [UNK] than american because he is [UNK] because his wife is because he [UNK] his life there because he [UNK] the [UNK] because he is just [UNK] [UNK] and doesnt like change dont ask [UNK] br possibly the man just does not care is not interested in [UNK] is a kind of an [UNK] he [UNK] that he is [UNK] in love with his wife he [UNK] of his [UNK] of two [UNK] he and his wife there is little to no [UNK] [UNK] his love for his wife in the movie it much more seems a [UNK] of [UNK] br on the [UNK] of an american [UNK] [UNK] [UNK] [UNK] to [UNK] [UNK] [UNK] [UNK] hate [UNK] to american [UNK] as a [UNK] for [UNK] [UNK] [UNK] to american [UNK] who read between the lines the [UNK] [UNK] with the [UNK] on a [UNK] [UNK] [UNK] is short [UNK] and [UNK] [UNK] the [UNK] to play along comes pretty easily with no [UNK] the [UNK] up to [UNK] seems to be [UNK] and apparently [UNK] br so now we have [UNK] [UNK] [UNK] himself over the air as the last free american the [UNK] for [UNK] secret [UNK] is fairly realistic and [UNK] although one [UNK] what happened when [UNK] [UNK] really and [UNK] had to [UNK] [UNK] etc must have [UNK] the [UNK] [UNK] anyway the [UNK] [UNK] the wife [UNK] [UNK] in the [UNK] for [UNK] [UNK] i never heard such [UNK] really happened on [UNK] [UNK] lines in [UNK] [UNK] [UNK] [UNK] says he goes to the [UNK] [UNK] but does not go is [UNK] by an american [UNK] who [UNK] his [UNK] how come is [UNK] to a [UNK] [UNK] in [UNK] is then released and [UNK] with the help of the [UNK] [UNK] [UNK] in the city of new [UNK] br and this is where the story really starts br br [UNK] from now on it is like a short story by paul [UNK] it is [UNK] [UNK] [UNK] lives in new york [UNK] as a [UNK] and [UNK] the [UNK] of his wife [UNK] really [UNK] or do they yes somehow they do and his [UNK] [UNK] some sort of [UNK] [UNK] [UNK] a [UNK] some american [UNK] [UNK] him and want him to be their [UNK] they even find his [UNK] dead wife for him or is she his wife anyway in the end [UNK] [UNK] [UNK] in at the [UNK] [UNK] and they [UNK] give him the big war [UNK] [UNK] [UNK] him in the [UNK] [UNK] to [UNK] [UNK] he [UNK] his life story and once this [UNK] [UNK] [UNK] himself on the [UNK] [UNK] without getting [UNK] the least [UNK] br while i can see that there must be an [UNK] of [UNK] and of [UNK] i just had the [UNK] that the main character is a person who at all times is pretty [UNK] to everything and hardly [UNK] of love for anyone so i found it difficult to [UNK] for this [UNK] who [UNK] his [UNK] [UNK] many [UNK] [UNK] on his [UNK] as a potential war hero having put his [UNK] at [UNK] for playing the last free american i [UNK] [UNK] to them this took a lot of [UNK] as a matter of fact however the movie [UNK] that by [UNK] the [UNK] [UNK] [UNK] for himself a [UNK] situation as he would have been [UNK] on the [UNK] side no matter who had [UNK] the war the [UNK] of his being [UNK] never comes up during the first part of the [UNK] br one might [UNK] that the whole story is a [UNK] fantasy and that [UNK] should [UNK] with [UNK] [UNK] or a [UNK] development of the story which [UNK] everything but even then it fails to make a point [UNK] i [UNK] because the love [UNK] in the [UNK] of two falls completely [UNK] this is a [UNK] especially if you [UNK] that the wife was played by [UNK] lee a [UNK] [UNK] and [UNK] actress she has much too little screen time and is forced to use a ridiculous [UNK] [UNK] another somehow [UNK] [UNK] are the different [UNK] [UNK] [UNK] and [UNK] [UNK] but i guess this is [UNK] [UNK] maybe i should give the book a chance                      \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "A_m5EmgA_EcR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "sample_text = ('awesome movie, I loved it so much')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCM9Fd9F_Me_",
        "outputId": "0c0e6cbc-ca7c-4e9e-d0d6-186b4065d731"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.50071293]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eVxVwFDN_XRy"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs=5,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8Myan3_aBk",
        "outputId": "b3cc9d50-2b96-4515-98eb-89a6c7510be1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 42s 90ms/step - loss: 0.6403 - accuracy: 0.6297 - val_loss: 0.4795 - val_accuracy: 0.7844\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 33s 83ms/step - loss: 0.4354 - accuracy: 0.8095 - val_loss: 0.3818 - val_accuracy: 0.8401\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 33s 83ms/step - loss: 0.3624 - accuracy: 0.8459 - val_loss: 0.3515 - val_accuracy: 0.8542\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.3306 - accuracy: 0.8620 - val_loss: 0.3312 - val_accuracy: 0.8594\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 33s 83ms/step - loss: 0.3181 - accuracy: 0.8674 - val_loss: 0.3352 - val_accuracy: 0.8583\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdbc2544b50>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxrWJ2H-_-nC",
        "outputId": "184f7ccb-905c-4b41-e963-cc9d377ff095"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 24s 61ms/step - loss: 0.3306 - accuracy: 0.8584\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33064761757850647, 0.8583599925041199]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_dataset)\n",
        "y_pred_classes = (y_pred>0.5)"
      ],
      "metadata": {
        "id": "Y-8g7MZfAKR8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1raIKCoL1O6",
        "outputId": "26e020e0-0dc6-41bb-c995-826253f4619c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       ...,\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[]\n",
        "for example, label in test_dataset:\n",
        "    y_test.extend(label.numpy())\n",
        "y_test\n",
        "len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iVf-VieERP2",
        "outputId": "2d0433bc-9d6e-4565-ee2b-0a724199a131"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKtZXMBmFeyN",
        "outputId": "804898d3-7f75-42cd-d2a0-d1973a2d946c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       ...,\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmNrJNFgPFrw",
        "outputId": "d6ff2b2a-66b4-4747-978d-32e6c6761c0e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "import numpy as np\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buNWlY7PDp55",
        "outputId": "21471654-8ccd-463f-8e30-83ba8b6c139d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.82      0.85     12500\n",
            "           1       0.83      0.90      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}